defaults:
    - agent: sac

env: cheetah_run

# this needs to be specified manually
experiment: test

num_train_steps: 1e6
replay_buffer_capacity: ${num_train_steps}


num_seed_steps: 5000

renormalize: False
renormalize_freq: 1000

eval_frequency: 10000
num_eval_episodes: 10
fixed_eval: true

device: cuda
action_repeat: 1


# logger
log_frequency: 10000
log_save_tb: true

# video recorder
save_video: true

# snapshot
save_frequency: 100000
save_latest: true
save_best_eval: true
save_zfill: 7
save_replay: false

seed: 1

# For debugging:
num_initial_states: null # Use a subset of initial states
max_episode_steps: null # If set, use shorter episodes


# hydra configuration
hydra:
    name: ${env}
    run:
        dir: ./exp/local/${now:%Y.%m.%d}/${now:%H%M}_${agent.name}_${experiment}
    sweep:
        dir: ./exp/${now:%Y.%m.%d}/${now:%H%M}_${agent.name}_${experiment}
        subdir: ${hydra.job.num}
        # subdir: seed_${seed}
    launcher:
        class: hydra_plugins.submitit.SubmititLauncher
        params:
            # one of auto,local,slurm and chronos
            queue: slurm

            folder: ${hydra.sweep.dir}/.${hydra.launcher.params.queue}
            queue_parameters:
                # slurm queue parameters
                slurm:
                    nodes: 1
                    num_gpus: 1
                    ntasks_per_node: 1
                    mem: ${hydra.launcher.mem_limit}GB
                    cpus_per_task: 10
                    time: 4320
                    # partition: learnfair
                    partition: priority
                    comment: For ICML
                    signal_delay_s: 120
                    job_name: ${hydra.job.name}
                # chronos queue parameters
                chronos:
                    # See crun documentation for most parameters
                    # https://our.internmc.facebook.com/intern/wiki/Chronos-c-binaries/crun/
                    hostgroup: fblearner_ash_bigsur_fair
                    cpu: 10
                    mem: ${hydra.launcher.mem_limit}
                    gpu: 1
                # local queue parameters
                local:
                    gpus_per_node: 1
                    tasks_per_node: 1
                    timeout_min: 60

        mem_limit: 64
        max_num_timeout: 1

#     launcher:
#         class: hydra_plugins.fairtask.FAIRTaskLauncher
#         params:
#             # debug launching issues, set to true to run workers in the same process.
#             no_workers: false
#             queue: slurm
#             queues:
#                 local:
#                     class: fairtask.local.LocalQueueConfig
#                     params:
#                         num_workers: 2
#                 slurm:
#                     class: fairtask_slurm.slurm.SLURMQueueConfig
#                     params:
#                         num_jobs: ${hydra.job.num_jobs}
#                         num_nodes_per_job: 1
#                         num_workers_per_node: 1
#                         name: ${hydra.name}.${experiment}
#                         maxtime_mins: 4320
#                         # partition: priority
#                         # comment: Small ICML exp
#                         partition: learnfair
#                         # partition: scavenge
#                         cpus_per_worker: 10
#                         mem_gb_per_worker: 128
#                         gres: 'gpu:volta:1'
#                         log_directory: ${hydra.sweep.dir}/.slurm
#                         output: slurm-%j.out
#                         error: slurm-%j.err
